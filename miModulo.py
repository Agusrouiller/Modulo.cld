# -*- coding: utf-8 -*-
"""Copia de Regresion-POO.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KptLnWjBnM4mN8jbSGphJMIC9OQyQl-I

### Desarrollo de nuestra librería de CdD3...

---

Hasta el momento hemos adquirido conocimientos sobre como generar nuestras propias clases, con sus respectivos atributos y métodos. Ahora es tiempo de pensar en desarrollar nuestra propia librería, utilizando todos los conceptos aprendidos y sumando algunos más que veremos en esta presentación.

Una librería puede estar compuesta de muchos archivos .py dependiendo el tamaño de la misma, pero en nuestro caso pondremos todas nuestras clases en un mismo archivo. Primero la vamos a desarrollar como una única celda de código en Colab y luego la exportaremos a un archivo .py, para luego importarlo y poder usarlo como cualquier otra librería (numpy, matplotlib, statsmodels, etc). La propuesta a seguir es la siguiente:

Preguntar lo de que una clase llame a la otra
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import statsmodels.api as sm
from scipy.stats import norm, expon, chi2,t, uniform, stats
from sklearn.metrics import auc

class AnalisisDescriptivo:
  """
  Clase para realizar análisis descriptivo de una muestra de datos.
  Permite construir histogramas, estimar densidades mediante núcleos (kernel),
  calcular el error cuadrático medio (ECM) y realizar gráficos QQ.
  """
  def __init__(self, datos):
    self.datos = np.array(datos)

  def genera_histograma(self, h):
    """Genera los intervalos y calcula las frecuencias relativas del histograma."""
    bins = np.arange(min(self.datos), max(self.datos) + h, h)
    fr_absolutas = np.zeros(len(bins) - 1)

    for dato in self.datos:
      for i in range(len(bins) - 1):
        if bins[i] <= dato < bins[i + 1]:
          fr_absolutas[i] += 1
          break

    frecuencias_intervalos = (fr_absolutas / len(self.datos)) / h
    return bins, frecuencias_intervalos

  def evalua_histograma(self, h, x):
    """Evalúa el histograma en puntos dados."""
    bins, frec = self.genera_histograma(h)
    res = np.zeros(len(x))

    for i, valor in enumerate(x):
      for j in range(len(bins) - 1):
        if bins[j] <= valor < bins[j + 1]:
          res[i] = frec[j]
          break

    return res # Retorna frecuencia relativa estimada en cada punto de x.

  # Estimación de densidad con kernels / núcleos
  def kernel_gaussiano(self, u):
    return (1 / np.sqrt(2 * np.pi)) * np.exp(-0.5 * u**2)

  def kernel_uniforme(self, u):
    return np.where(np.abs(u) <= 0.5, 1, 0)

  def epanechnikov(self, u):
    return 3/4 * (1 - u**2) * (np.abs(u) <= 1)

  def triangular(self, u):
    return (1 + u) * (u >= -1) * (u < 0) + (1 - u) * (u >= 0) * (u <= 1)

  def densidad_nucleo(self, x, h, kernel):
    """Calcula la estimación de densidad con diferentes kernels."""
    n = len(self.datos)
    density = np.zeros_like(x)

    for i, xi in enumerate(x):
      u = (self.datos - xi) / h  # Escalamiento de los datos
      if kernel == "uniforme":
        density[i] = np.sum(self.kernel_uniforme(u)) / (n * h)
      elif kernel == "gaussiano":
        density[i] = np.sum(self.kernel_gaussiano(u)) / (n * h)
      elif kernel == "epanechnikov":
        density[i] = np.sum(self.epanechnikov(u)) / (n * h)
      elif kernel == "triangular":
        density[i] = np.sum(self.triangular(u)) / (n * h)

    return density #Densidad estimada en cada punto de x.

  def calcular_ecm(self, x, h, kernel, densidad_teorica): #calcula el ecm para un h especifico
    """Calcula el ECM entre la densidad estimada con núcleo y la densidad teórica."""
    if kernel == "histograma":
      densidad_estimada = self.evalua_histograma(h, x)
      return np.mean((densidad_estimada - densidad_teorica) ** 2)
    else:
      densidad_estimada = self.densidad_nucleo(x, h, kernel)
      return np.mean((densidad_estimada - densidad_teorica) ** 2)

  def buscar_h_optimo(self, x, h_values, kernel, densidad_teorica):  #busca h otimo y me da el ecm para cada valor de h
    """Busca el h que minimiza el ECM entre la densidad estimada y la teórica."""
    errores = [self.calcular_ecm(x, h, kernel, densidad_teorica) for h in h_values]
    h_optimo = h_values[np.argmin(errores)]
    return h_optimo, errores

  #Estimación de QQ plot
  def miqqplot(self):
    x = np.array(self.datos)
    x_ord = np.sort(x)
    n = len(x_ord)

    # Establecer los cuantiles teóricos de la distribución normal estándar
    p_teoricos = np.arange(1, (n+1)) / (n+1)
    cuantiles_teoricos = norm.ppf(p_teoricos)

    #datos estandarizados, cuantil muestral
    media_x_ord = np.mean(x_ord)
    desvio_x_ord = np.std(x_ord)
    x_ord_s = (x_ord - media_x_ord)/desvio_x_ord

    plt.scatter(cuantiles_teoricos, x_ord_s, label="Cuantiles muestrales")
    plt.plot(cuantiles_teoricos, cuantiles_teoricos, color="red", label="Recta identidad (y=x)")

    plt.xlabel("Cuantiles teóricos (Normal estándar)")
    plt.ylabel("Cuantiles muestrales")
    plt.title("Gráfico QQ - Comparación de cuantiles")
    plt.legend()
    plt.grid(True)
    plt.show()
  pass


class GeneradoraDeDatos:
  """
  Clase para generar datos simulados de distintas distribuciones teóricas
  (Normal, Exponencial, Chi-cuadrado, t de Student, Uniforme y una distribución BS).
  Devuelve tanto los datos simulados como la función de densidad teórica asociada.
  """
  def __init__(self, n):
    self.n = n

  def normal(self, media=0, desviacion=1):
    """Genera datos normales y devuelve también la densidad teórica."""
    datos = np.random.normal(media, desviacion, size=self.n)
    def densidad(x): return norm.pdf(x, media, desviacion)
    return datos, densidad

  def exponencial(self, beta=1):
    """Genera datos exponenciales y devuelve también la densidad teórica."""
    datos = np.random.exponential(scale=beta, size=self.n)
    def densidad(x): return (1/beta) * np.exp(-x/beta) * (x >= 0)
    return datos, densidad

  def chi_cuadrado(self, df):
    """Genera datos chi-cuadrado y devuelve también la densidad teórica."""
    datos = np.random.chisquare(df=df, size=self.n)
    def densidad(x): return chi2.pdf(x, df)
    return datos, densidad

  def BS(self):
    """Genera datos con distribución BS y devuelve su densidad teórica."""
    u = np.random.uniform(size=self.n)
    y = np.copy(u)
    ind = np.where(u > 0.5)[0]
    y[ind] = np.random.normal(0, 1, size=len(ind))
    for j in range(5):
        ind = np.where((u > j * 0.1) & (u <= (j+1) * 0.1))[0]
        y[ind] = np.random.normal(j/2 - 1, 1/10, size=len(ind))

    def densidad(x):
      d = 0.5 * norm.pdf(x, 0, 1)
      for j in range(5):
        d += 0.1 * norm.pdf(x, j/2 - 1, 1/10)
      return d

    return y, densidad

  def t_student(self, df):
    """Genera datos con distribución t de Student y devuelve la densidad teórica."""
    datos = np.random.standard_t(df, size=self.n)
    def densidad(x): return t.pdf(x, df)
    return datos, densidad

  def uniforme(self, a=0, b=1):
    """Genera datos con distribución uniforme y devuelve la densidad teórica."""
    datos = np.random.uniform(low=a, high=b, size=self.n)
    def densidad(x): return uniform.pdf(x, loc=a, scale=b-a)
    return datos, densidad
  pass

class RegresionLinealSimple():
  """
  Clase para ajustar un modelo de regresión lineal simple y realizar inferencia estadística.
  Calcula parámetros estimados (pendiente y ordenada al origen), varianza, residuos, errores estándar,
  valores t, p-valores, intervalos de confianza y recta ajustada.
  """

  def __init__(self, x, y):
    self.x = np.array(x)
    self.y = np.array(y)
    self.n = len(self.x)
    self.gl = self.n - 2
    self.b1 = self.calcular_pendiente()
    self.b0 = self.calcular_ordenada()
    self.varianza = self.estimar_varianza()
    self.SE_b0, self.SE_b1 = self.calcular_errores_estandar()
    self.t_b0, self.t_b1 = self.calcular_t_obs()
    self.p_b0, self.p_b1 = self.calcular_p_valores()
    self.IC_b0, self.IC_b1 = self.calcular_intervalos_confianza()

    self.modelo = sm.OLS(self.y, sm.add_constant(self.x))
    self.resultado = self.modelo.fit()
    self.betas = self.resultado.params

  def calcular_pendiente(self):
    x_bar = np.mean(self.x)
    y_bar = np.mean(self.y)
    numerador = np.sum((self.x - x_bar) * (self.y - y_bar))
    denominador = np.sum((self.x - x_bar) ** 2)
    return numerador / denominador

  def calcular_ordenada(self):
    x_bar = np.mean(self.x)
    y_bar = np.mean(self.y)
    return y_bar - self.b1 * x_bar

  def recta_ajustada(self, x_valores):
    return self.b0 + self.b1 * x_valores

  def obtener_residuos(self):
    return self.y - self.recta_ajustada(self.x)

  def estimar_varianza(self):
    residuos = self.obtener_residuos()
    return np.sum(residuos**2) / self.gl

  def calcular_errores_estandar(self):
    x_bar = np.mean(self.x)
    Sxx = np.sum((self.x - x_bar)**2)
    SE_b1 = np.sqrt(self.varianza / Sxx) # Error estándar de beta1
    SE_b0 = np.sqrt(self.varianza * (1/self.n + x_bar**2 / Sxx)) # Error estándar de beta0
    return SE_b0, SE_b1

  def calcular_t_obs(self, hipotesis_b0=0, hipotesis_b1=0):
    t_b0 = (self.b0 - hipotesis_b0) / self.SE_b0
    t_b1 = (self.b1 - hipotesis_b1) / self.SE_b1
    return t_b0, t_b1

  def calcular_valor_critico(self, alpha=0.05):
    return stats.t.ppf(1 - alpha/2, df=self.gl)

  def calcular_p_valores(self, hipotesis_b0=0, hipotesis_b1=0):
    t_b0, t_b1 = self.calcular_t_obs(hipotesis_b0, hipotesis_b1)
    p_b0 = 2 * (1 - stats.t.cdf(abs(t_b0), df=self.gl))
    p_b1 = 2 * (1 - stats.t.cdf(abs(t_b1), df=self.gl))
    return p_b0, p_b1

  def calcular_intervalos_confianza(self, alpha=0.05):
    t_crit = self.calcular_valor_critico(alpha)
    IC_b0 = (self.b0 - t_crit * self.SE_b0, self.b0 + t_crit * self.SE_b0)
    IC_b1 = (self.b1 - t_crit * self.SE_b1, self.b1 + t_crit * self.SE_b1)
    return IC_b0, IC_b1

  def predecir(self, x_nuevos, alpha=0.05):
    """
    Calcula la predicción de Y ajustado, el intervalo de confianza para la media de Y
    y el intervalo de predicción para un nuevo valor de Y dado x_nuevos.
    """
    X_nuevos = np.array(x_nuevos)
    y_ajustado = np.dot(x_nuevos, self.betas)

    prediccion = self.resultado.get_prediction(x_nuevos)
    ic = prediccion.conf_int(alpha=alpha)
    ip = prediccion.conf_int(obs=True, alpha=alpha)
    diccionario = {
    'res': [print(f"La recta ajustada es: {y_ajustado:.4f}")],
    'int_conf': [print(f"intervalo de confianza: [{ic[0, 0]:.4f}, {ic[0, 1]:.4f}]")],
    'int_pred': [print(f"intervalo de predicción: [{ip[0, 0]:.4f}, {ip[0, 1]:.4f}]")],
    }
    return diccionario

  def graficar_residuos_y_qqplot(self, x):
      y_ajustada = self.recta_ajustada(x)
      residuos = self.obtener_residuos()

      # Gráfico de residuos vs valores ajustados
      plt.scatter(y_ajustada, residuos, color='purple')
      plt.axhline(0, color='gray', linestyle='--')
      plt.xlabel("Valores ajustados (ŷ)")
      plt.ylabel("Residuos (r)")
      plt.title("Gráfico de residuos vs valores ajustados")
      plt.grid(True)
      plt.show()

      # QQ-Plot usando la clase Estimacion
      estimador = AnalisisDescriptivo(residuos)
      estimador.miqqplot()

  def __concluir_sobre_hipotesis(self, coeficiente='b1', hipotesis=0, alpha=0.05):
    t_obs_tuple = self.calcular_t_obs(hipotesis_b0=hipotesis, hipotesis_b1=hipotesis)
    p_valor_tuple = self.calcular_p_valores(hipotesis_b0=hipotesis, hipotesis_b1=hipotesis)
    intervalo_tuple = self.calcular_intervalos_confianza(alpha=alpha)
    if coeficiente == 'b1':
      t_obs = t_obs_tuple[1]
      p_valor = p_valor_tuple[1]
      IC = intervalo_tuple[1]
      estimado = self.b1
    elif coeficiente == 'b0':
      t_obs = t_obs_tuple[0]
      p_valor = p_valor_tuple[0]
      IC = intervalo_tuple[0]
      estimado = self.b0
    else:
      raise ValueError("El coeficiente debe ser 'b0' o 'b1'.")

    t_crit = self.calcular_valor_critico(alpha)

    print(f"\n--- Hipótesis sobre {coeficiente} ---")
    print(f"H₀: {coeficiente} = {hipotesis}")
    print(f"Estimación puntual: {estimado:.4f}")
    print(f"t observado: {t_obs:.4f}")
    print(f"t crítico (α = {alpha}): ±{t_crit:.4f}")
    print(f"p-valor: {p_valor:.4f}")
    print(f"Intervalo de confianza al {(1-alpha)*100:.0f}%: ({IC[0]:.4f}, {IC[1]:.4f})")

    # Conclusión por región de rechazo
    if abs(t_obs) > t_crit:
      print("➤ Conclusión (región de rechazo): Se rechaza H₀.")
    else:
      print("➤ Conclusión (región de rechazo): No se puede rechazar H₀.")

    # Conclusión por intervalo
    if IC[0] <= hipotesis <= IC[1]:
      print("➤ Conclusión (intervalo de confianza): El valor hipotético está dentro del intervalo.")
    else:
      print("➤ Conclusión (intervalo de confianza): El valor hipotético está fuera del intervalo.")

  def graficar(self, ax=None):
    if ax is None:
        fig, ax = plt.subplots()
    ax.scatter(self.x, self.y, color='blue')
    ax.plot(self.x, self.recta_ajustada(self.x), color='black')
    ax.set_title("Recta de regresión lineal")
    ax.set_xlabel("x")
    ax.set_ylabel("y")
  pass

class RegresionLinealMultiple:
  """
  Clase para ajustar un modelo de regresión lineal múltiple y realizar inferencia estadística.
  Calcula coeficientes estimados, varianza residual, residuos, errores estándar, valores t,
  p-valores, intervalos de confianza, matriz de varianza-covarianza y métricas de ajuste como R² y R² ajustado.
  Incluye herramientas para diagnóstico del modelo mediante gráficos y pruebas de significancia conjunta.
  """
  def __init__(self, X, y):
    self.X = np.array(X)
    self.y = np.array(y)
    self.n = len(self.y)
    self.k = self.X.shape[1]  # número de predictores
    self.gl = self.n - self.k - 1

    self.modelo = sm.OLS(self.y, self.X) #define el modelo de regresion lineal
    self.resultado = self.modelo.fit() #arma el esquema
    self.betas = self.resultado.params # Coeficientes
    self.errors_estandar = self.resultado.bse # Errores estándar
    self.t_values = self.resultado.tvalues # Estadísticos t
    self.p_values = self.resultado.pvalues  # p-valores
    self.intervalos_confianza = self.resultado.conf_int() #calcula los intevalos de confianza para los betas

  def predecir(self, x_nuevos, alpha=0.05):
    """
    Calcula la predicción de Y ajustado, el intervalo de confianza para la media de Y
    y el intervalo de predicción para un nuevo valor de Y dado x_nuevos.
    """
    X_nuevos = np.array(x_nuevos)
    y_ajustado = np.dot(x_nuevos, self.betas)

    prediccion = self.resultado.get_prediction(x_nuevos)
    ic = prediccion.conf_int(alpha=alpha)
    ip = prediccion.conf_int(obs=True, alpha=alpha)
    diccionario = {
    'res': [print(f"La recta ajustada es: {y_ajustado:.4f}")],
    'int_conf': [print(f"intervalo de confianza: [{ic[0, 0]:.4f}, {ic[0, 1]:.4f}]")],
    'int_pred': [print(f"intervalo de predicción: [{ip[0, 0]:.4f}, {ip[0, 1]:.4f}]")],
    }
    return diccionario

  def obtener_residuos(self):
    return self.resultado.resid

  def estimar_varianza(self):
    return self.resultado.mse_resid

  def mostrar_R2(self):
    r2 = self.resultado.0
    r2_adj = self.resultado.rsquared_adj
    print(f"R²: {r2:.4f}")
    print(f"R² ajustado: {r2_adj:.4f}")
    return r2, r2_adj

  def calcular_intervalos_confianza(self, alpha=0.05): #calcula los intervalos de confianza para los betas con distinto alpha
    return self.resultado.conf_int(alpha=alpha)

  def __concluir_sobre_hipotesis(self, indice_coef, hipotesis=0, alpha=0.05):
    nombre = "Intercepto" if indice_coef == 0 else f"b{indice_coef}"
    estimado = self.betas[indice_coef]
    SE = self.errors_estandar[indice_coef]
    t_obs = self.t_values[indice_coef]
    p_valor = self.p_values[indice_coef]
    IC = self.intervalos_confianza[indice_coef]

    print(f"\n--- Hipótesis sobre {nombre} ---")
    print(f"H₀: {nombre} = {hipotesis}")
    print(f"Estimación puntual: {estimado:.4f}")
    print(f"p-valor: {p_valor:.4f}")
    print(f"Intervalo de confianza al {(1-alpha)*100:.0f}%: ({IC[0]:.4f}, {IC[1]:.4f})")

    if p_valor < alpha:
        print("➤ Conclusión (región de rechazo): Se rechaza H₀.")
    else:
        print("➤ Conclusión (región de rechazo): No se puede rechazar H₀.")

    if IC[0] <= hipotesis <= IC[1]:
        print("➤ Conclusión (intervalo de confianza): El valor hipotético está dentro del intervalo.")
    else:
        print("➤ Conclusión (intervalo de confianza): El valor hipotético está fuera del intervalo.")

  def resumen(self):
    print(self.resultado.summary())

  def graficar_residuos_y_qqplot_mult(self, x):
    betas = self.betas
    y_ajustada = np.dot(self.X, betas)
    residuos = self.obtener_residuos()

    # Gráfico de residuos vs valores ajustados
    plt.scatter(y_ajustada, residuos, color='purple')
    plt.axhline(0, color='gray', linestyle='--')
    plt.xlabel("Valores ajustados (ŷ)")
    plt.ylabel("Residuos (r)")
    plt.title("Gráfico de residuos vs valores ajustados")
    plt.grid(True)
    plt.show()

    # QQ-Plot usando la clase Estimacion
    estimador = AnalisisDescriptivo(residuos)
    estimador.miqqplot()
  pass

class Regresionlogistica:
  def __init__(self, x_train, x_test, y_train, y_test):
    self.x_train = sm.add_constant(np.array(x_train), has_constant='add')
    self.x_test = sm.add_constant(np.array(x_test), has_constant='add')
    self.y_train = np.array(y_train)
    self.y_test = np.array(y_test)

  def ajustar_modelo(self):
    self.modelo = sm.Logit(self.y_train, self.x_train)
    self.resultado = self.modelo.fit()
    print(self.resultado.summary())

  def predecir(self, umbral=0.5):
    self.probabilidades_test = self.resultado.predict(self.x_test)
    self.y_pred = 1 * (self.probabilidades_test >= umbral)
    return self.y_pred

  def evaluar(self, corte=0.5):
    y_predic = self.predecir(umbral=corte)
    a = np.sum((y_predic == 1) & (self.y_test == 1))  # Verdaderos positivos
    b = np.sum((y_predic == 1) & (self.y_test == 0))  # Falsos positivos
    c = np.sum((y_predic == 0) & (self.y_test == 1))  # Falsos negativos
    d = np.sum((y_predic == 0) & (self.y_test == 0))  # Verdaderos negativos

    error = (b + c) / len(self.y_test)

    matriz_error = pd.DataFrame({'y_test=1': [a, c, a+c],
                                  'y_test=0': [b, d, b+d],
                                  "total": [a+b, c+d, a+b+c+d]},
                                  index=["y_pred=1", "y_pred=0", "total"])

    sensibilidad = a / (a + c)
    especificidad = d / (b + d)
    prop_falsos_negativos = c / (a + c)
    prop_falsos_positivos = b / (b + d)

    print(matriz_error)
    print(f'Error de mala clasificación: {error:.3f}')
    print(f"Sensibilidad: {sensibilidad:.3f}")
    print(f"Especificidad: {especificidad:.3f}")
    print(f"Proporción de falsos negativos: {prop_falsos_negativos:.3f}")
    print(f"Proporción de falsos positivos: {prop_falsos_positivos:.3f}")

  def encontrar_p_optimo_y_graficar_roc(self):
    p_values = np.linspace(0, 1, 100)
    sensibilidad = []
    especificidad = []

    for p in p_values:
        y_pred_temp = 1 * (self.probabilidades_test >= p)
        a = np.sum((y_pred_temp == 1) & (self.y_test == 1))
        b = np.sum((y_pred_temp == 1) & (self.y_test == 0))
        c = np.sum((y_pred_temp == 0) & (self.y_test == 1))
        d = np.sum((y_pred_temp == 0) & (self.y_test == 0))

        sens = a / (a + c) if (a + c) != 0 else 0
        espec = d / (b + d) if (b + d) != 0 else 0

        sensibilidad.append(sens)
        especificidad.append(espec)

    sensibilidad = np.array(sensibilidad)
    especificidad = np.array(especificidad)
    youden = sensibilidad + especificidad - 1
    indice_optimo = np.argmax(youden)

    self.p_optimo = p_values[indice_optimo]
    sens_optima = sensibilidad[indice_optimo]
    espec_optima = especificidad[indice_optimo]
    roc_auc = auc(1 - especificidad, sensibilidad)

    print(f"Mejor punto de corte p: {self.p_optimo:.4f}")
    print(f"Sensibilidad: {sens_optima:.4f}")
    print(f"Especificidad: {espec_optima:.4f}")
    if roc_auc > 0.9:
      print(f"El clasificador es excelente debido a un AUC = {roc_auc:.3f}")
    elif 0.8 < roc_auc <= 0.9:
      print(f"El clasificador es bueno debido a un AUC = {roc_auc:.3f}")
    elif 0.7 < roc_auc <= 0.8:
      print(f"El clasificador es regular debido a un AUC = {roc_auc:.3f}")
    else:
      print(f"El clasificador es fallido/pobre debido a un AUC = {roc_auc:.3f}")

    # Gráfico
    plt.plot(1 - especificidad, sensibilidad, color='blue', label='Curva ROC')
    plt.scatter(1 - espec_optima, sens_optima, color='red', s=100, edgecolors='black', label='Punto óptimo')
    plt.xlabel('1 - Especificidad')
    plt.ylabel('Sensibilidad')
    plt.title('Curva ROC con punto óptimo')
    plt.legend()
    plt.grid(True)
    plt.show()
  pass

"""Las 2 primeras clases del módulo podrían considerarse ya finalizadas, pero a partir de conceptos que agregaremos a continuación, tal vez se te ocurran modificaciones que mejoren su comportamiento. Por otro lado, tendrás que diseñar de la mejor manera posible la relación entre las clases "Regresion", "RegresionLineal" y "RegresionLogistica" para lograr todas las tareas y cuentas ejecutadas a medida que se desarrollaron los temas.

# Sumemos algunos conceptos más...

En el contexto de la Programación Orientada a Objetos, hemos aprendido a utilizar **Herencia** y **Polimorfismo** en nuestro código. Para seguir mejorando la "calidad" de nuestros desarrollos, será deseable que nuestra librería cumpla con las siguientes características:

*  **Cohesión**: en el diseño de un método/función, es importante pensar bien la tarea que debe realizar, intentando que sea única y bien definida. Cuantas más cosas diferentes haga una función sin relación entre sí, más complicado será el código de entender. Decimos entonces que nuestros métodos poseen una "cohesión débil" si nuestras líneas de código no persiguen una única funcionalidad. Por el contrario, la "cohesión fuerte" de nuestros algoritmos (que debe ser nuestro objetivo al diseñar programas) indica que existe una alta relación entre los elementos existentes dentro del módulo.

  ```
  # Cohesión débil
  def suma():
    num1 = float(input("Dame primer número"))
    num2 = float(input("Dame segundo número"))
    suma = num1 + num2
    print(suma)

  # Cohesión fuerte
  def suma(numeros):
    total = 0
    for i in numeros:
        total = total + i
    return total
  ```

*  **Acoplamiento**: mide la dependencia entre dos módulos distintos de software, como pueden ser por ejemplo dos clases. "Acoplamiento fuerte" indica que un módulo tiene dependencias internas con otros. Por el contrario, "Acoplamiento débil", indica que no existe dependencia de un módulo con otros. Esta última característica debería ser la meta de nuestro software. El término acoplamiento está muy relacionado con la cohesión, ya que acoplamiento débil suele ir ligado a cohesión fuerte.

  ```
  class Clase1:
    x = True
    pass

  class Clase2:
    def mi_metodo(self, valor):
        if Clase1.x:
            self.valor = valor

  mi_clase = Clase2()
  mi_clase.mi_metodo("Hola")
  mi_clase.valor

  Clase1.x = False
  mi_clase.mi_metodo("Chau")
  mi_clase.valor
  ```

*  **Reproducibilidad**: se refiere a la capacidad de obtener los mismos resultados a partir de un algoritmo cuando se repiten las mismas condiciones iniciales. Es decir, si el desarrollador aplica el mismo algoritmo a los mismos datos de entrada, debería llegar a los mismos datos de salida.  

  ```
  import numpy as np

  def algoritmo_no_reproducible():
    # np.random.seed(10)
    datos = np.random.rand(5)
    resultado = datos.mean()
    return resultados

  print(algoritmo_no_reproducible())
  print(algoritmo_no_reproducible())
  ```

* **Encapsulamiento**: probablemente sea el concepto más interesante de los 4 enumerados. Se refiere a la restricción u ocultamiento del acceso a ciertos datos de un objeto por fuera de la clase. Dicho de otra manera, encapsular consiste en hacer que un conjunto de atributos o métodos internos a una clase no se puedan acceder ni modificar desde fuera, sino que tan solo el propio objeto pueda acceder a ellos. En Python, el encapsulamiento se implementa mediante convenciones de nomenclatura para los atributos y métodos de una clase. Aunque Python no tiene una forma estricta de hacer atributos verdaderamente privados, utiliza convenciones para indicar el nivel de acceso:

 -> *Público*: sin guiones bajos. Atributos y métodos pueden ser accedidos desde cualquier parte.

 -> *Protegido*: un guion bajo previo al nombre. Se utiliza para indicar que el atributo o método no debe ser accedido directamente aunque es posible.

 -> *Privado*: doble guion bajo previo al nombre. Aquí Python realiza una alteración al nombre para dificultar el acceso directo.

  ```
  class Persona:
    def __init__(self, nombre, edad):
        self.nombre = nombre  # Atributo público
        self._edad = edad     # Atributo protegido
        self.__dinero = 1000  # Atributo privado

    def mostrar_datos(self):
        print(f"Nombre: {self.nombre}, Edad: {self._edad}, Dinero: {self.__dinero}")

    def __incrementar_dinero(self, cantidad):
        self.__dinero += cantidad

  persona = Persona("Juan", 30)
  print(persona.nombre)        # Acceso permitido
  print(persona._edad)         # Acceso permitido (pero no recomendado)
  # print(persona.__dinero)    # Esto dará un error

  persona.mostrar_datos()       # Acceso permitido, muestra todos los datos
  # persona.__incrementar_dinero(500) # Esto dará un error  
  ```

  Pero pero...

  ```
  print(persona._Persona__dinero)  # Acceso mediante name mangling
  persona._Persona__incrementar_dinero(500)  # Acceso al método privado mediante "name mangling"
  persona.mostrar_datos()  
  ```

    Aunque no se vean a simple vista, siempre están visibles los atributos y métodos pero con un nombre distinto, para de alguna manera ocultarlos y evitar su uso. Igualmente podremos llamarlos, pero por lo general no es una buena idea.

### Manos a la obra...

Para resolver problemas de Regresión Lineal, las funcionalidades que tuvimos que implementar fueron:
* Graficar la dispersión de puntos (en base a los datos) y la recta de mejor ajuste (en el caso de ser más de una variable predictora cuantitativa, graficar cada una de ellas vs la respuesta).
* Calcular coeficiente de correlación entre la variable predictora y la variable respuesta.
* Ajustar el modelo mediante mínimos cuadrados y estimar los parámetros correspondientes.
* Realizar el análisis de los residuos (implica calcular los residuos puntuales, realizar un gráfico qqPlot y un gráfico de los residuos vs los valores predichos).
* Ajustar el modelo utilizando la librería statsmodels.
* Almacenar y retornar los valores de los betas, errores standars, t_obs y p-valor.
* Predecir valores de respuesta ante nuevas entradas.
* Calcular intervalos de confianza y de predicción.
* Calcular coeficiente de determinación y R cuadrado ajustado de un determinado modelo.

Para resolver problemas de Regresión Logística, las funcionalidades que tuvimos que implementar fueron:
* Ajustar el modelo utilizando la librería statsmodels, permitiendo setear el porcentaje de datos para realizar el ajuste.
* Almacenar y retornar los valores de los betas, errores standars, t_obs y p-valor.
* A partir de los datos de test, poder calcular matriz de confusión, error total de mala clasificación, sensibilidad y especificidad.
* Predecir valores de respuesta ante nuevas entradas utilizando un umbral.
* Realizar la curva ROC, calcular el área bajo la curva y evaluar el clasificador de acuerdo en la tabla vista en teoría.

En base a las funcionalidades enumeradas, es momento de pensar como diseñar nuestras clases, teniendo en cuenta las operaciones que son comunes a ambas clases y aquellas que son particulares.

Una vez implementadas las clases, deben intentar resolver nuevamente las operaciones ya planteadas en los Colab anteriores, evitando repetir todos los fragmentos de código que seguramente han tenido que reutilizar.
"""